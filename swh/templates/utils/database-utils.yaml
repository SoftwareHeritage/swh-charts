---
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-utils
  namespace: {{ $.Values.namespace }}
data:
  register-task-types.sh: |
    #!/usr/bin/env bash

    set -eux

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh scheduler task-type register
  register-webhook-event-types.sh: |
    #!/usr/bin/env bash

    set -ex

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh webhooks event-type register-defaults

    # List results
    swh webhooks event-type list
  register-webhook-event-type-endpoints.sh: |
    #!/usr/bin/env bash

    set -ex

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh webhooks endpoint create origin.visit "$URL" --secret "${TOKEN}"

    swh webhooks endpoint list origin.visit

  register-scrubber-configuration.sh: |
    #!/usr/bin/env bash

    set -eux

    # Note: The subcommand swh uses internally the environment variable
    # SWH_CONFIG_FILENAME

    # Usage: swh scrubber check init [OPTIONS] {storage|journal|objstorage}
    #
    #   Initialise a scrubber check configuration for the datastore defined in the
    #   configuration file and given object_type.
    #
    #   A checker configuration configuration consists simply in a set of:
    #
    #   - backend: the datastore type being scrubbed (storage, objstorage or
    #   journal),
    #
    #   - object-type: the type of object being checked,
    #
    #   - nb-partitions: the number of partitions the hash space is divided   in;
    #   must be a power of 2,
    #
    #   - name: an unique name for easier reference,
    #
    #   - check-hashes: flag (default to True) to select the hash validation step
    #   for   this scrubbing configuration,
    #
    #   - check-references: flag (default to True for storage and False for the
    #   journal   backend) to select the reference validation step for this
    #   scrubbing configuration.
    #
    # Options:
    #   --object-type [snapshot|revision|release|directory|content]
    #   --nb-partitions INTEGER
    #   --name TEXT
    #   --check-hashes / --no-check-hashes
    #   --check-references / --no-check-references
    #   -h, --help                      Show this message and exit.

    extra_cmd=""
    [ ! -z "${NB_PARTITIONS}" ] && extra_cmd="${extra_cmd} --nb-partitions $NB_PARTITIONS"
    [ "${CHECK_HASHES}" = "false" ] && extra_cmd="${extra_cmd} --no-check-hashes"
    [ "${CHECK_REFERENCES}" = "false" ] && extra_cmd="${extra_cmd} --no-check-references"

    # Check whether the configuration already exists (the subcommand script is not
    # idempotent)
    config_exists=$(swh scrubber check list | grep $NAME | awk '{print substr($2,1,length($2)-1)}')

    if [ "${config_exists}" = "${NAME}" ]; then
        echo "Configuration ${NAME} already exists in scrubber, do nothing"
        exit 0
    fi

    swh scrubber check init \
      --name $NAME \
      --object-type $OBJECT_TYPE \
      $extra_cmd \
      $BACKEND

  init-keyspace.py: |
    from swh.core import config
    from swh.storage.cassandra import create_keyspace

    def get_cassandra_config(storage_config):
        if storage_config["cls"] == 'cassandra' :
            return storage_config

        if storage_config["cls"] == 'pipeline':
            cassandra_config = storage_config["steps"][-1]
            if cassandra_config["cls"] != "cassandra":
                raise ValueError(
                    "Misconfigured pipeline, the last step must be the actual "
                    "cassandra storage configuration."
                )

            return cassandra_config

        raise ValueError(
            "Misconfigured storage configuration. It must be either a <pipeline> "
            "or a <cassandra> storage instance."
        )

    full_config = config.read('/etc/swh/config.yml')
    storage_config = full_config["storage"]
    cassandra_conf = get_cassandra_config(storage_config)
    hosts = cassandra_conf.get("hosts")
    if not hosts:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<hosts> key must be provided."
        )

    auth_provider = cassandra_conf.get("auth_provider")
    if not auth_provider:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<auth_provider> key must be provided."
        )

    keyspace = cassandra_conf.get("keyspace")
    if not keyspace:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<keyspace> key must be provided."
        )

    create_keyspace(hosts=hosts, keyspace=keyspace, auth_provider=auth_provider)

  extract-storage-postgresql-config-py: |
    import yaml
    from swh.core import config

    def get_postgresql_config(storage_config):
      if storage_config["cls"] == 'postgresql' :
        return storage_config

      if storage_config["cls"] == 'pipeline':
        for config in storage_config["steps"]:
          c = get_postgresql_config(config)
          if c:
            return c

      return None

    full_config = config.read('/etc/swh/config.yml')

    storage_config = full_config["storage"]

    postgresql_conf = get_postgresql_config(storage_config)

    if postgresql_conf is None:
      print("No postgresql configuration found!\n")
      exit(1)

    f = open("/tmp/config.yml", "w")
    f.write(yaml.dump({"storage": postgresql_conf}))

  check-backend-version.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full storage configuration
      # possibly with a pipeline (only storage allows this).
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | \
      tee "${TEMP_FILE}"

    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim it
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ -e "${CODE_VERSION}" ]; then
      echo "Unable to find the code version"
      exit 1
    fi

    if [ -e "${DB_VERSION}" ]; then
      echo "Unable to find the code version"
      exit 1
    fi

    if [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      echo "Code and DB versions are different. Blocking the deployment"
      exit 1
    fi

  migrate-backend.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full configuration
      # possibly with a pipeline
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | tee "${TEMP_FILE}"

    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim it
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ "${DB_VERSION}" = "None" ]; then
      echo "The database should be initialized..."

    elif [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      swh db --config-file=$CONFIG_FILE upgrade $EXTRA_CMD "${MODULE_NAME}"

    else
      echo "The database is initialized and up-to-date, nothing to do!"
      echo "Continue with the deployment."

    fi

  initialize-backend.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full configuration
      # possibly with a pipeline
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | \
      tee "${TEMP_FILE}"

    set -x
    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ "${DB_VERSION}" = "None" ]; then
      # This must be run as "postgres" user (for pg extensions installation)
      uri=postgresql://postgres:$SWH_PGPASSWORD@$SWH_PGHOST:5432/$SWH_PGDATABASE
      swh db init-admin --db-name=$uri "${MODULE}"
      # This must be run with the owner of the db
      swh db --config-file=$CONFIG_FILE init $EXTRA_CMD "${MODULE}"

    elif [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      echo "Code and DB versions are different."

    else
      echo "The database is initialized and up-to-date, nothing to do!"
      echo "Continue with the deployment."
    fi
