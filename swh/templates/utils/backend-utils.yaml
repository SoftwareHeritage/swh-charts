---
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-utils
  namespace: {{ $.Values.namespace }}
data:
  graph-prepare-memory-volume.sh: |
    #!/usr/bin/env bash
    # Uses env variables WITNESS_FILE, DATASET_SOURCE, DATASET_LOCATION, GRAPH_NAME

    set -eux

    [ -z "${DATASET_LOCATION}" ] && \
      echo "<DATASET_LOCATION> env variable must be set" && exit 1
    [ -z "${DATASET_SOURCE}" ] && \
      echo "<DATASET_SOURCE> env variable must be set" && exit 1
    [ -z "${WITNESS_FILE}" ] && \
      echo "<WITNESS_FILE> env variable must be set" && exit 1
    [ -z "${WITNESS_SOURCE_FILE}" ] && \
      echo "<WITNESS_SOURCE_FILE> env variable must be set" && exit 1
    [ -z "${PERIOD}" ] && \
      echo "<PERIOD> env variable must be set" && exit 1
    [ -z "${GRAPH_NAME}" ] && \
      echo "<GRAPH_NAME> env variable must be set" && exit 1

    [ -f ${WITNESS_FILE} ] && echo "Graph ready, do nothing." && exit 0

    while [ ! -f ${WITNESS_SOURCE_FILE} ]; do
        echo "${WITNESS_SOURCE_FILE} not present, wait for it to prepare the graph dataset..."
        sleep $PERIOD
    done

    # Create empty dataset location destination for copy to be ok
    mkdir -p ${DATASET_LOCATION}

    graph_stats=${GRAPH_NAME}.stats
    # Symlink all files from dataset source to the destination (including the *.graph)
    [ -L "${DATASET_LOCATION}/${graph_stats}" ] || \
      ln -sf ${DATASET_SOURCE}/* ${DATASET_LOCATION}/

    graph_name=${GRAPH_NAME}.graph
    # We hard-copy the *.graph file
    if [ -L "${DATASET_LOCATION}/${graph_name}" ] || ! [ -f ${DATASET_LOCATION}/${graph_name} ]; then
      cp -v --remove-destination ${DATASET_SOURCE}/${graph_name} ${DATASET_LOCATION}/;
    fi

    graph_transposed_name=${GRAPH_NAME}-transposed.graph
    if [ -L ${DATASET_LOCATION}/${graph_transposed_name} ] || ! [ -f ${DATASET_LOCATION}/${graph_transposed_name} ]; then
      cp -v --remove-destination ${DATASET_SOURCE}/${graph_transposed_name} ${DATASET_LOCATION}/;
    fi

    # Finally, we make explicit the graph is ready
    touch ${WITNESS_FILE}

  graph-wait-for-dataset.sh: |
    #!/usr/bin/env bash
    # Uses env variables WITNESS_FILE
    [ -z "${WITNESS_FILE}" ] && \
      echo "<WITNESS_FILE> env variable must be set" && exit 1

    while [ ! -f ${WITNESS_FILE} ]; do
        echo "${WITNESS_FILE} not present, wait for it to start the graph..."
        sleep $PERIOD
    done

  graph-fetch-dataset.sh: |
    #!/usr/bin/env bash
    # Uses env variables WITNESS_FILE, DATASET_LOCATION, DATASET_NAME, GRAPH_NAME
    [ -z "${DATASET_LOCATION}" ] && \
      echo "<DATASET_LOCATION> env variable must be set" && exit 1
    [ -z "${DATASET_NAME}" ] && \
      echo "<DATASET_NAME> env variable must be set" && exit 1
    [ -z "${WITNESS_FILE}" ] && \
      echo "<WITNESS_FILE> env variable must be set" && exit 1
    [ -z "${GRAPH_NAME}" ] && \
      echo "<GRAPH_NAME> env variable must be set" && exit 1

    set -eux

    [ -f ${WITNESS_FILE} ] && \
        echo "Dataset <${DATASET_NAME}> already present. Skip." && \
        exit 0

    case "${DATASET_NAME}" in
        test|example)
            # For test (or example) dataset sample, clone the source repository of
            # swh.graph and use the example dataset within
            git clone \
              --depth 1 \
              https://gitlab.softwareheritage.org/swh/devel/swh-graph.git/ \
              /tmp/swh-graph
            # Create empty dataset location destination for copy to be ok
            mkdir -p ${DATASET_LOCATION} && rmdir ${DATASET_LOCATION}
            # Actual copy of the test dataset
            cp -r /tmp/swh-graph/swh/graph/example_dataset/compressed \
                  ${DATASET_LOCATION}

            # Make explicit the graph is ready
            touch ${WITNESS_FILE}
        ;;
        *)
            # Otherwise, download the dataset locally
            swh graph download \
              --name ${DATASET_NAME} \
              ${DATASET_LOCATION}

            # Reindex graph dataset (for those anterior to 2024). This should not be
            # necessary for most recent graph datasets.

            # For old datasets missing a .ef though, this just fails with
            # `2024-09-02T14:11:56.190692004Z graph-rpc-python3k 0: Cannot map
            # Elias-Fano pointer list .../graph.ef`, so we trigger a reindex step
            reindex_witness_file=${DATASET_LOCATION}/${GRAPH_NAME}.ef
            [ ! -f $reindex_witness_file ] && \
              swh graph reindex ${DATASET_LOCATION}/${GRAPH_NAME}

            # Make explicit the graph is ready
            touch ${WITNESS_FILE}
        ;;
    esac

  graph-reindex-dataset.sh: |
    #!/usr/bin/env bash
    [ -z "${DATASET_LOCATION}" ] && \
      echo "<DATASET_LOCATION> env variable must be set" && exit 1
    [ -z "${GRAPH_NAME}" ] && \
      echo "<GRAPH_NAME> env variable must be set" && exit 1

    # For old datasets missing a .ef or in the wrong format, this fails with
    # `Cannot map Elias-Fano pointer list .../graph.ef`. The solution is to
    # reindex the dataset
    swh graph reindex ${DATASET_LOCATION}/${GRAPH_NAME}

  initialize-search-backend.sh: |
    #!/usr/bin/env bash

    set -eux

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh search initialize
  register-task-types.sh: |
    #!/usr/bin/env bash

    set -eux

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh scheduler task-type register
  register-listing-task.sh: |
    #!/usr/bin/env bash

    # For test or sandboxed environment only
    # This passes along a forge type and url to schedule for listing at
    # startup

    set -eux

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    ( swh scheduler task list \
      --task-type $SWH_FORGE_TYPE --policy=oneshot 2>&1 | grep $SWH_FORGE_URL ) \
      || swh scheduler task add $SWH_FORGE_TYPE \
           url=$SWH_FORGE_URL \
             --policy=oneshot
  register-webhook-event-types.sh: |
    #!/usr/bin/env bash

    set -ex

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh webhooks event-type register-defaults

    # List results
    swh webhooks event-type list
  register-webhook-event-type-endpoints.sh: |
    #!/usr/bin/env bash

    set -ex

    # Uses internally the environment variable SWH_CONFIG_FILENAME
    swh webhooks endpoint create origin.visit "$URL" --secret "${TOKEN}"

    swh webhooks endpoint list origin.visit

  register-scrubber-configuration.sh: |
    #!/usr/bin/env bash

    set -eux

    # Note: The subcommand swh uses internally the environment variable
    # SWH_CONFIG_FILENAME

    # Usage: swh scrubber check init [OPTIONS] {storage|journal|objstorage}
    #
    #   Initialise a scrubber check configuration for the datastore defined in the
    #   configuration file and given object_type.
    #
    #   A checker configuration configuration consists simply in a set of:
    #
    #   - backend: the datastore type being scrubbed (storage, objstorage or
    #   journal),
    #
    #   - object-type: the type of object being checked,
    #
    #   - nb-partitions: the number of partitions the hash space is divided   in;
    #   must be a power of 2,
    #
    #   - name: an unique name for easier reference,
    #
    #   - check-hashes: flag (default to True) to select the hash validation step
    #   for   this scrubbing configuration,
    #
    #   - check-references: flag (default to True for storage and False for the
    #   journal   backend) to select the reference validation step for this
    #   scrubbing configuration.
    #
    # Options:
    #   --object-type [snapshot|revision|release|directory|content]
    #   --nb-partitions INTEGER
    #   --name TEXT
    #   --check-hashes / --no-check-hashes
    #   --check-references / --no-check-references
    #   -h, --help                      Show this message and exit.

    extra_cmd=""
    [ ! -z "${NB_PARTITIONS}" ] && extra_cmd="${extra_cmd} --nb-partitions $NB_PARTITIONS"
    [ "${CHECK_HASHES}" = "false" ] && extra_cmd="${extra_cmd} --no-check-hashes"
    [ "${CHECK_REFERENCES}" = "false" ] && extra_cmd="${extra_cmd} --no-check-references"

    # Check whether the configuration already exists (the subcommand script is
    # not idempotent). Note that this requires a storage configuration entry
    # key in the SWH_CONFIG_FILENAME
    config_exists=$(swh scrubber check list | grep $NAME | awk '{print substr($2,1,length($2)-1)}')

    if [ "${config_exists}" = "${NAME}" ]; then
        echo "Configuration ${NAME} already exists in scrubber, do nothing"
        exit 0
    fi

    swh scrubber check init \
      --name $NAME \
      --object-type $OBJECT_TYPE \
      $extra_cmd \
      $BACKEND

  init-keyspace.py: |
    from swh.core import config
    from swh.storage.cassandra import create_keyspace

    def get_cassandra_config(storage_config):
        if storage_config["cls"] == 'cassandra' :
            return storage_config

        if storage_config["cls"] == 'pipeline':
            cassandra_config = storage_config["steps"][-1]
            if cassandra_config["cls"] != "cassandra":
                raise ValueError(
                    "Misconfigured pipeline, the last step must be the actual "
                    "cassandra storage configuration."
                )

            return cassandra_config

        raise ValueError(
            "Misconfigured storage configuration. It must be either a <pipeline> "
            "or a <cassandra> storage instance."
        )

    full_config = config.read('/etc/swh/config.yml')
    storage_config = full_config["storage"]
    cassandra_conf = get_cassandra_config(storage_config)
    hosts = cassandra_conf.get("hosts")
    if not hosts:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<hosts> key must be provided."
        )

    auth_provider = cassandra_conf.get("auth_provider")
    if not auth_provider:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<auth_provider> key must be provided."
        )

    keyspace = cassandra_conf.get("keyspace")
    if not keyspace:
        raise ValueError(
            "Misconfigured cassandra configuration, "
            "<keyspace> key must be provided."
        )

    create_keyspace(hosts=hosts, keyspace=keyspace, auth_provider=auth_provider)

  extract-storage-postgresql-config-py: |
    import yaml
    from swh.core import config

    def get_postgresql_config(storage_config):
      if storage_config["cls"] == 'postgresql' :
        return storage_config

      if storage_config["cls"] == 'pipeline':
        for config in storage_config["steps"]:
          c = get_postgresql_config(config)
          if c:
            return c

      return None

    full_config = config.read('/etc/swh/config.yml')

    storage_config = full_config["storage"]

    postgresql_conf = get_postgresql_config(storage_config)

    if postgresql_conf is None:
      print("No postgresql configuration found!\n")
      exit(1)

    f = open("/tmp/config.yml", "w")
    f.write(yaml.dump({"storage": postgresql_conf}))

  check-backend-version.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full storage configuration
      # possibly with a pipeline (only storage allows this).
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | \
      tee "${TEMP_FILE}"

    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim it
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ -e "${CODE_VERSION}" ]; then
      echo "Unable to find the code version"
      exit 1
    fi

    if [ -e "${DB_VERSION}" ]; then
      echo "Unable to find the code version"
      exit 1
    fi

    if [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      echo "Code and DB versions are different. Blocking the deployment"
      exit 1
    fi

  migrate-backend.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full configuration
      # possibly with a pipeline
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | tee "${TEMP_FILE}"

    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim it
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ "${DB_VERSION}" = "None" ]; then
      echo "The database should be initialized..."

    elif [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      swh db --config-file=$CONFIG_FILE upgrade $EXTRA_CMD "${MODULE_NAME}"

    else
      echo "The database is initialized and up-to-date, nothing to do!"
      echo "Continue with the deployment."

    fi

  initialize-backend.sh: |
    #!/usr/bin/env bash

    set -eu

    TEMP_FILE=/tmp/db-version.txt
    CONFIG_FILE=$SWH_CONFIG_FILENAME
    EXTRA_CMD=""

    if [ -z ${MODULE} ]; then
      echo The env variable must be defined with the module to check
      echo for example "storage"
      exit 1
    fi

    if [ ! -z $MODULE_CONFIG_KEY ]; then
      EXTRA_CMD="--module-config-key=$MODULE_CONFIG_KEY"
    fi

    if [ "${MODULE}" = "storage" ]; then
      # extracting the postgresql configuration from a full configuration
      # possibly with a pipeline
      set +e
      python /entrypoints/extract-storage-postgresql-config-py || exit 0
      set -e
      CONFIG_FILE=/tmp/config.yml
    fi

    # checking the database status
    swh db --config-file=$CONFIG_FILE version "${MODULE}" $EXTRA_CMD | \
      tee "${TEMP_FILE}"

    set -x
    CODE_VERSION=$(awk -F':' '/code / {print $2}' ${TEMP_FILE})
    # trim
    CODE_VERSION=${CODE_VERSION#"${CODE_VERSION%%[![:space:]]*}"}

    DB_VERSION=$(awk -F':' '/^version: / {print $2}' ${TEMP_FILE})
    # trim
    DB_VERSION=${DB_VERSION#"${DB_VERSION%%[![:space:]]*}"}

    if [ "${DB_VERSION}" = "None" ]; then
      # This must be run as "postgres" user (for pg extensions installation)
      uri=postgresql://postgres:$SWH_PGPASSWORD@$SWH_PGHOST:5432/$SWH_PGDATABASE
      swh db init-admin --db-name=$uri "${MODULE}"
      # This must be run with the owner of the db
      swh db --config-file=$CONFIG_FILE init $EXTRA_CMD "${MODULE}"

    elif [ "$DB_VERSION" != "$CODE_VERSION" ]; then
      echo "Code and DB versions are different."

    else
      echo "The database is initialized and up-to-date, nothing to do!"
      echo "Continue with the deployment."
    fi
