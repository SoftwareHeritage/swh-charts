---
# Should be set
# clusterName: ""

# Main common configuration for the chart.
namespace: cluster-components

# All applications whose 'enabled' flag is false by default are applications that are
# already installed in the cluster by the main swh provisioning except for the clusters
# minikube (local dev) and rancher (admin cluster).
cert-manager:
  enabled: false
  # Supported in the chart, not seen on the pods...
  priorityClassName: cluster-components-system

# Whether to activate metallb and allocate some ipAddressPool
metallb:
  enabled: false
  # ipAddressPool:
  #   - ip1
  #   - ip2

prometheus:
  enabled: false
  # Not working somehow... Charts reference it but it's not seen in minikube
  priorityClassName: cluster-components-system
  namespaceOverride: cattle-monitoring-system
  grafana:
    namespaceOverride: cattle-monitoring-system
  prometheus-node-exporter:
    namespaceOverride: cattle-monitoring-system
  kube-state-metrics:
    namespaceOverride: cattle-monitoring-system

# This configuration is swh specific (and independent from the prometheus configuration
# already done during terraform provisioning). When activated, this allows to relay the
# cluster's prometheus alerts to the cluster admin's alertmanager ingress irc relay
alertmanagerConfig:
  enabled: false
  namespace: cattle-monitoring-system
  ircRelayHost: https://alertmanager-irc-relay.internal.admin.swh.network/swh-sysadm
  # .htaccess or authentication credentials
  authentication:
    enabled: true
    secretRef: alertmanager-irc-relay-config
    userKeyRef: user
    passwordKeyRef: password
  # inhibitorRules:
  #   - targetMatch:
  #     - name: mylabel
  #       value: myvalue
  #     ...

alertmanagerIrcRelay:
  enabled: false
  priorityClassName: cluster-components-system
  ingress:
    enabled: true
    hosts:
      - alertmanager-irc-relay.admin.swh.network
      - alertmanager-irc-relay.internal.admin.swh.network
    # secret holding the .htpasswd information
    authentication: ingress-nginx/basic-auth
    tls:
      enabled: true
      # clusterIssuer: letsencrypt-production
  http_port: 8000
  # Room to connect to
  room: swh-sysadm
  # requestedMemory: "128Mi"
  # requestedCpu: "500"
  # optional
  # limitedMemory: "256Mi"
  # limitedCpu: "1000"

blackboxExporter:
  enabled: false
  priorityClassName: cluster-components-system
  nameOverride: blackbox-exporter
  namespaceOverride: cattle-monitoring-system
  serviceMonitor:
    enabled: true
    selfMonitor:
      enabled: true
    prometheusRule:
      enabled: true
  pspEnabled: false
  config:
    modules:
      swh_www:
        prober: http
        timeout: 5s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
          follow_redirects: true
          preferred_ip_protocol: "ip4"
          fail_if_body_not_matches_regexp:
            - '<title>Software Heritage</title>'
      icmp:
        prober: icmp
        icmp:
          preferred_ip_protocol: ip4
  # This config is needed for ICMP probing: you need NET_RAW to send ICMP
  # packets as non-root.
  securityContext:
    readOnlyRootFilesystem: true
    allowPrivilegeEscalation: false
    capabilities:
      add: ["NET_RAW"]

podPriority:
  enabled: false
  priorities:
    cluster-components-system:
      range: 50000-100000
      value: 75000
      description: Highest pod priorities (ingress, operator, collector, controller)

svix:
  enabled: false
  # Svix-server version must match the svix python library
  # in the swh-webhooks image's requirements.
  version: v1.23
  ingress:
    host: svix.example.org
    createTLS: true
  requestedMemory: 100Mi
  requestedCpu: 100m
  namespace: svix-server
  postgres:
    requestedMemory: 100Mi
    requestedCpu: 100m
    persistentVolume: false
  redis:
    requestedMemory: 100Mi
    requestedCpu: 100m
    persistentVolume: false
    # dsn: svix-redis
    # podSelectorPolicy:
    #   matchLabels:
    #     app: redis-svix-leader

alerting:
  enabled: false
  period:
    microDelay: 2m
    tinyDelay: 5m
    smallDelay: 15m
  cassandra:
    unrepairedSize: 214748364800
  saveCodeNow:
    aggregatedPeriod: 1h
    threshold: 10
  runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"
  ingress:
    errorRate: 10
    slowDownRate: 90
    slowDownPeriod: 10000

scrapeExternalMetrics:
  enabled: false
  interval: 30s
  deployments:
    # rabbitmq:
    #   interval: 60s
    #   namespace: rabbitmq
    #   port: 9419
    #   ips:
    #     - 192.168.130.50
    # cassandra:
    #   namespace: cassandra
    #   metricsName: jmx
    #   serviceMonitorName: cassandra-jmx-exporter
    #   port: 7070
    #   ips:
    #     - 192.168.130.181
    #     - 192.168.130.182
    #     - 192.168.130.183

# Configuration for the local docker registry cache
dockerCache:
  enabled: false
  priorityClassName: cluster-components-system
  namespace: docker-cache
  ingress:
    hosts:
      - docker-cache.admin.swh.network
    tls:
      enabled: false
      # clusterIssuer: letsencrypt-production
  imageName: registry
  imageVersion: latest
  imagePullPolicy: Always
  storageRequest: 10Gi
  storageClassName: ceph-rbd
  metrics:
    enabled: true
  instances:
    docker.io:
      enabled: true
      remoteUrl: https://registry-1.docker.io
      # httpPrefix: /docker.io/
      # storageRequest: 5Gi
      # storageClassName: ceph-cephfs
    swh:
      enabled: true
      remoteUrl: https://container-registry.softwareheritage.org
    registry.k8s.io:
      enabled: true
      # metrics:
      #   enabled:
      #     false
    quay.io:
      enabled: false
    ghcr.io:
      enabled: false

cloudnativePg:
  enabled: false
  nameOverride: cloudnative-pg
  # secrets must be stored in the same namespace
  # namespace: cnpg
  # Specific postgresql setup (can be overridden per instance)
  # postgresql:
  #   parameters:
  #     max_worker_processes: "60"
  #   pg_hba:
  #     - host all all all md5
  # Specific storage to use to persist data (can be overridden per instance)
  # storage:
  #   storageClass: local-persistent
  #   size: 1Gi
  # Deploys as many postgresql clusters in the deployment section
  deployments:
    # Name of the cluster is the key followed by its configuration
    # cluster-pg-name:
    #   # whether to enable the <cluster-pg-name> deployment
    #   enabled: false
    #   # whether it's a testing node (if so, disables some security checks for
    #   # drain/uncordon operations), false by default
    #   testing: false
    #   # nb of replicas
    #   instances: 1
    #   # affinity: {}
    #   # nodeSelector: {}
    #   # (optional) barman object store configuration reference if backups are enabled
    #   barmanObjectStoreRef: minioTestObjectStore
    #   # (optional) Upstream cluster to start the dbs from
    #   externalClusterRef: clusterPgStagingDb1
    #   # Whether we enable a pooler (pgbouncher) for access
    #   pooler:
    #     # Whether to activate a pgbouncer service (useful for accessing that pg
    #     # cluster instance from outside the kubernetes cluster via a load-balancer)
    #     enabled: true
    #     default_pool_size: "10"
    #     max_client_conn: "1000"
    #     instances: 3
    #     type: rw
    #   # To initialize the dbs to be managed within the new pg cluster
    #   initdb:
    #     # Whether to initialize some dbs from another (external) pg cluster
    #     enabled: true
    #     # type:
    #     # - microservice: destination cluster is designed to host a single app
    #     # database owned by the specified application user (recommended)
    #     # - monolith: destination cluster is designed to host multiple databases
    #     # and different users, imported from the source cluster
    #     type: monolith
    #     dbs:
    #     - swh-deposit
    #     - swh-scheduler
    #     - swh-web
    #     - swh-vault
    #     # - swh-blocking
    #     # - swh-masking
    #     source: cluster-pg-staging-db1
    #   # Whether to enable regular backups
    #   backup:
    #     enabled: true
    #     retention: "30d"
    #     name: daily-midnight
    #     # every day at midnight
    #     cron: "0 0 0 * * *"

rabbitmq:
  enabled: false
  namespace: rabbitmq
  # Number of replicas per deployment instance (can be overridden per instance config)
  # replicas: 1
  # The storage volumes to use (can be overridden per instance config)
  # storageVolume:
  #   class: local-persistent
  #   size: "1Gi"
  # The resources requests and limits (can be overridden per deployment instance)
  # requestedMemory: 1Gi
  # requestedCpu: 500m
  # limitedMemory: 2Gi
  # limitedCpu: 1000m
  # logLevel: info
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: "swh/backend"
  #           operator: In
  #           values:
  #           - "true"
  # deployments:
  #   # name of the desired instance
  #   scheduler:
  #     enabled: true

kafka:
  enabled: false
  namespace: kafka-system
  # Number of replicas (can be overridden per instance)
  # replicas: 1
  # Replication factor
  # replicationFactor: 2
  # minInSyncReplicas: 1
  deployments:
    # cluster-name:
    #   # zookeeper:
    #   #   storage:
    #   #     type: persistent-claim
    #   #     size: 1Gi
    #   #     deleteClaim: false
    #   # # Either a standalone kafka instance with a jbod setup
    #   # # storage:
    #   # #   type: jbod
    #   # #   volumes:
    #   # #   - id: 0
    #   # #     type: persistent-claim
    #   # #     size: 1Gi
    #   # #     deleteClaim: false
    #   # #   - id: 1
    #   # #     type: persistent-claim
    #   # #     size: 1Gi
    #   # #     deleteClaim: false
    #   # or a pool of kafka nodes with their own replicas and storage
    #   # # pools:
    #   # #   pool-a:
    #   # #     # replicas: 3
    #   # #     storage:
    #   # #       type: jbod
    #   # #       volumes:
    #   # #         - id: 0
    #   # #           type: persistent-claim
    #   # #           size: 1Gi
    #   # #           deleteClaim: false
    #   # #   pool-b:
    #   # #     storage:
    #   # #       type: jbod
    #   # #       volumes:
    #   # #         - id: 0
    #   # #           type: persistent-claim
    #   # #           size: 1Gi
    #   # #           deleteClaim: false

cassandra:
  enabled: false
  namespace: cassandra-system
  deployments:
    # cluster-name and configuration
    # cassandra-cluster:
    #   datacenters:
    #     - name: dc1
    #       replicas: 3

elasticsearch:
  enabled: false
  namespace: swh
  # Our actual version
  # ii  elasticsearch 7.15.2 amd64 Distributed RESTful search engine
  # Can be overriden per deployment
  version: 7.15.2
  # most recent as of today
  # version: 8.14.2
  # Can be overriden per deployment
  # nodeName: node
  # Common volume claim templates to deal with storage information
  # Can be overriden per instance
  # volumeClaimTemplates:
  # - metadata:
  #     # Do not change this name unless you set up a volume mount for the
  #     # data path.
  #     name: elasticsearch-data
  #   spec:
  #     accessModes:
  #     - ReadWriteOnce
  #     resources:
  #       requests:
  #         storage: 5Gi
  #     storageClassName: local-path
  deployments:
    # cluster-name and configuration
    # search:
    #   replicas: 1

redis:
  enabled: false
  # Namespace (overridable per instance)
  namespace: redis
  # [optional] extra labels
  labels: {}
  # [optional] Node selection query (overridable per instance)
  # nodeSelector:
  #   kubernetes.io/hostname: minikube
  # [optional] Affinity (overridable per instance)
  # affinity:
  #  nodeAffinity:
  #    requiredDuringSchedulingIgnoredDuringExecution:
  #      nodeSelectorTerms:
  #      - matchExpressions:
  #        - key: svix-server
  #          operator: In
  #          values:
  #          - "true"
  # [optional] Priority class to use (overridable per instance)
  # priorityClassName: swh-storages
  # [optional] Activate service monitor (overridable per instance)
  serviceMonitor:
    enabled: false
  # [optional] Activate redis exporter (overridable per instance)
  redisExporter:
    enabled: false
    image: quay.io/opstree/redis-exporter
    tag: "v1.44.0"
    imagePullPolicy: IfNotPresent
    resources: {}
      # requests:
      #   cpu: 100m
      #   memory: 128Mi
      # limits:
      #   cpu: 100m
      #   memory: 128Mi
    env: []
    # - name: REDIS_EXPORTER_INCL_SYSTEM_METRICS
    #   value: "true"
    # - name: UI_PROPERTIES_FILE_NAME
    #   valueFrom:
    #     configMapKeyRef:
    #       name: game-demo
    #       key: ui_properties_file_name
    # - name: SECRET_USERNAME
    #   valueFrom:
    #     secretKeyRef:
    #       name: mysecret
    #       key: username
    # [optional] Add extra redis configuration (overridable per instance)
  extraConfig:
    enabled: false
  #   data: |
  #     save 60 500
  #     appendonly yes
  #     appendfsync everysec
  # Actual instance deployment
  # deployments:
  #   # name of the deployment, appName will be called redis-counters (unless
  #   # appName is specified)
  #   counters:
  #     # [optional] Application name override (e.g. to call it "redis" for example)
  #     # appName: redis
  #     replicas: 3
