---
# Should be set
# clusterName: ""

# Main common configuration for the chart.
namespace: cluster-components

# All applications whose 'enabled' flag is false by default are applications that are
# already installed in the cluster by the main swh provisioning except for the clusters
# minikube (local dev) and rancher (admin cluster).
cert-manager:
  enabled: false
  # Supported in the chart, not seen on the pods...
  priorityClassName: cluster-components-system

# Whether to activate metallb and allocate some ipAddressPool
metallb:
  enabled: false
  # ipAddressPool:
  #   - ip1
  #   - ip2

prometheus:
  enabled: false
  # Not working somehow... Charts reference it but it's not seen in minikube
  priorityClassName: cluster-components-system
  namespaceOverride: cattle-monitoring-system
  grafana:
    namespaceOverride: cattle-monitoring-system
  prometheus-node-exporter:
    namespaceOverride: cattle-monitoring-system
  kube-state-metrics:
    namespaceOverride: cattle-monitoring-system

# This configuration is swh specific (and independent from the prometheus configuration
# already done during terraform provisioning). When activated, this allows to relay the
# cluster's prometheus alerts to the cluster admin's alertmanager ingress irc relay
alertmanagerConfig:
  enabled: false
  namespace: cattle-monitoring-system
  ircRelayHost: https://alertmanager-irc-relay.internal.admin.swh.network/swh-sysadm
  # .htaccess or authentication credentials
  authentication:
    enabled: true
    secretRef: alertmanager-irc-relay-config
    userKeyRef: user
    passwordKeyRef: password
  # inhibitorRules:
  #   - targetMatch:
  #     - name: mylabel
  #       value: myvalue
  #     ...

alertmanagerIrcRelay:
  enabled: false
  priorityClassName: cluster-components-system
  ingress:
    enabled: true
    hosts:
      - alertmanager-irc-relay.admin.swh.network
      - alertmanager-irc-relay.internal.admin.swh.network
    # secret holding the .htpasswd information
    authentication: ingress-nginx/basic-auth
    tls:
      enabled: true
      # clusterIssuer: letsencrypt-production
  http_port: 8000
  # Room to connect to
  room: swh-sysadm
  # requestedMemory: "128Mi"
  # requestedCpu: "500"
  # optional
  # limitedMemory: "256Mi"
  # limitedCpu: "1000"

blackboxExporter:
  enabled: false
  priorityClassName: cluster-components-system
  nameOverride: blackbox-exporter
  namespaceOverride: cattle-monitoring-system
  serviceMonitor:
    enabled: true
    selfMonitor:
      enabled: true
    prometheusRule:
      enabled: true
  pspEnabled: false
  config:
    modules:
      swh_www:
        prober: http
        timeout: 5s
        http:
          valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
          follow_redirects: true
          preferred_ip_protocol: "ip4"
          fail_if_body_not_matches_regexp:
            - '<title>Software Heritage</title>'

podPriority:
  enabled: false
  priorities:
    cluster-components-system:
      range: 50000-100000
      value: 75000
      description: Highest pod priorities (ingress, operator, collector, controller)

svix:
  enabled: false
  # Svix-server version must match the svix python library
  # in the swh-webhooks image's requirements.
  version: v1.23
  ingress:
    host: svix.example.org
    createTLS: true
  requestedMemory: 100Mi
  requestedCpu: 100m
  namespace: svix-server
  postgres:
    requestedMemory: 100Mi
    requestedCpu: 100m
    persistentVolume: false
  redis:
    # If false, the redis is not deployed through the chart but through
    # the k8s-cluster-config repository configuration
    managedByChart: false
    requestedMemory: 100Mi
    requestedCpu: 100m
    persistentVolume: false
    # dsn: svix-redis
    # podSelectorPolicy:
    #   matchLabels:
    #     app: redis-svix-leader

alerting:
  enabled: false
  period:
    tinyDelay: 5m
    smallDelay: 15m
  cassandra:
    unrepairedSize: 214748364800
  saveCodeNow:
    aggregatedPeriod: 1h
    threshold: 10
  runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

scrapeExternalMetrics:
  enabled: false
  interval: 30s
  deployments:
    # rabbitmq:
    #   interval: 60s
    #   namespace: rabbitmq
    #   port: 9419
    #   ips:
    #     - 192.168.130.50
    # cassandra:
    #   namespace: cassandra
    #   metricsName: jmx
    #   serviceMonitorName: cassandra-jmx-exporter
    #   port: 7070
    #   ips:
    #     - 192.168.130.181
    #     - 192.168.130.182
    #     - 192.168.130.183

# Configuration for the local docker registry cache
dockerCache:
  enabled: false
  priorityClassName: cluster-components-system
  namespace: docker-cache
  ingress:
    hosts:
      - docker-cache.admin.swh.network
    tls:
      enabled: false
      # clusterIssuer: letsencrypt-production
  imageName: registry
  imageVersion: latest
  imagePullPolicy: Always
  storageRequest: 10Gi
  storageClassName: ceph-rbd
  metrics:
    enabled: true
  instances:
    docker.io:
      enabled: true
      remoteUrl: https://registry-1.docker.io
      # httpPrefix: /docker.io/
      # storageRequest: 5Gi
      # storageClassName: ceph-cephfs
    swh:
      enabled: true
      remoteUrl: https://container-registry.softwareheritage.org
    registry.k8s.io:
      enabled: true
      # metrics:
      #   enabled:
      #     false
    quay.io:
      enabled: false
    ghcr.io:
      enabled: false

cloudnativePg:
  enabled: false
  nameOverride: cloudnative-pg
  # secrets must be stored in the same namespace
  # namespace: cnpg
  # Specific postgresql setup (can be overridden per instance)
  # postgresql:
  #   parameters:
  #     max_worker_processes: "60"
  #   pg_hba:
  #     - host all all all md5
  # Specific storage to use to persist data (can be overridden per instance)
  # storage:
  #   storageClass: local-persistent
  #   size: 1Gi
  # Deploys as many postgresql clusters in the deployment section
  deployments:
    # Name of the cluster is the key followed by its configuration
    # cluster-pg-name:
    #   # whether to enable the <cluster-pg-name> deployment
    #   enabled: false
    #   # whether it's a testing node (if so, disables some security checks for
    #   # drain/uncordon operations), false by default
    #   testing: false
    #   # nb of replicas
    #   instances: 1
    #   # affinity: {}
    #   # nodeSelector: {}
    #   # (optional) barman object store configuration reference if backups are enabled
    #   barmanObjectStoreRef: minioTestObjectStore
    #   # (optional) Upstream cluster to start the dbs from
    #   externalClusterRef: clusterPgStagingDb1
    #   # Whether we enable a pooler (pgbouncher) for access
    #   pooler:
    #     # Whether to activate a pgbouncer service (useful for accessing that pg
    #     # cluster instance from outside the kubernetes cluster via a load-balancer)
    #     enabled: true
    #     default_pool_size: "10"
    #     max_client_conn: "1000"
    #     instances: 3
    #     type: rw
    #   # To initialize the dbs to be managed within the new pg cluster
    #   initdb:
    #     # Whether to initialize some dbs from another (external) pg cluster
    #     enabled: true
    #     # type:
    #     # - microservice: destination cluster is designed to host a single app
    #     # database owned by the specified application user (recommended)
    #     # - monolith: destination cluster is designed to host multiple databases
    #     # and different users, imported from the source cluster
    #     type: monolith
    #     dbs:
    #     - swh-deposit
    #     - swh-scheduler
    #     - swh-web
    #     - swh-vault
    #     # - swh-blocking
    #     # - swh-masking
    #     source: cluster-pg-staging-db1
    #   # Whether to enable regular backups
    #   backup:
    #     enabled: true
    #     retention: "30d"
    #     name: daily-midnight
    #     # every day at midnight
    #     cron: "0 0 0 * * *"

rabbitmq:
  enabled: false
  namespace: rabbitmq
  # Number of replicas per deployment instance (can be overridden per instance config)
  # replicas: 1
  # The storage volumes to use (can be overridden per instance config)
  # storageVolume:
  #   class: local-persistent
  #   size: "1Gi"
  # The resources requests and limits (can be overridden per deployment instance)
  # requestedMemory: 1Gi
  # requestedCpu: 500m
  # limitedMemory: 2Gi
  # limitedCpu: 1000m
  # logLevel: info
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: "swh/backend"
  #           operator: In
  #           values:
  #           - "true"
  # deployments:
  #   # name of the desired instance
  #   scheduler:
  #     enabled: true
